{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "modelTrainPreClassified.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7KYj8EtoPxZ"
      },
      "source": [
        "# Constants:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsusFSopoVfn"
      },
      "source": [
        "SEQ_LEN = 240   # how many past candles to use to predict\n",
        "CANDLES_SHIFT = 2 # how many candles to shift between sequences\n",
        "NAME = \"r20t0\"\n",
        "VALIDATION_PCT = 0.2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zspc4aD1juaP"
      },
      "source": [
        "DF initialisation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "KfSNaRUgirE3",
        "outputId": "e9f3c791-099e-496a-9b0d-7aa70dfe4a48"
      },
      "source": [
        "import pandas as pd\n",
        "main_df = pd.read_csv(\"HistoricalDataClassified.csv\")\n",
        "main_df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BTC_close</th>\n",
              "      <th>BTC_low</th>\n",
              "      <th>BTC_high</th>\n",
              "      <th>BTC_volume</th>\n",
              "      <th>BTC_average</th>\n",
              "      <th>BTC_HLPercent</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>675.500000</td>\n",
              "      <td>675.500000</td>\n",
              "      <td>675.500000</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>675.500000</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>667.000000</td>\n",
              "      <td>667.000000</td>\n",
              "      <td>671.000000</td>\n",
              "      <td>4.137774</td>\n",
              "      <td>668.041259</td>\n",
              "      <td>0.005961</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>670.100000</td>\n",
              "      <td>670.100000</td>\n",
              "      <td>672.000000</td>\n",
              "      <td>13.203878</td>\n",
              "      <td>671.999856</td>\n",
              "      <td>0.002827</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>667.000007</td>\n",
              "      <td>667.000007</td>\n",
              "      <td>672.000000</td>\n",
              "      <td>0.304313</td>\n",
              "      <td>671.903346</td>\n",
              "      <td>0.007440</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>667.000007</td>\n",
              "      <td>667.000007</td>\n",
              "      <td>667.000007</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>667.000007</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403218</th>\n",
              "      <td>8824.457823</td>\n",
              "      <td>8814.000000</td>\n",
              "      <td>8828.100000</td>\n",
              "      <td>2.099438</td>\n",
              "      <td>8825.360378</td>\n",
              "      <td>0.001597</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403219</th>\n",
              "      <td>8780.138300</td>\n",
              "      <td>8772.888273</td>\n",
              "      <td>8823.000000</td>\n",
              "      <td>18.767512</td>\n",
              "      <td>8794.423289</td>\n",
              "      <td>0.005680</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403220</th>\n",
              "      <td>8776.869900</td>\n",
              "      <td>8773.414687</td>\n",
              "      <td>8790.000000</td>\n",
              "      <td>4.273303</td>\n",
              "      <td>8783.317655</td>\n",
              "      <td>0.001887</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403221</th>\n",
              "      <td>8758.555000</td>\n",
              "      <td>8732.293856</td>\n",
              "      <td>8772.888415</td>\n",
              "      <td>11.747596</td>\n",
              "      <td>8753.805186</td>\n",
              "      <td>0.004627</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403222</th>\n",
              "      <td>8730.000000</td>\n",
              "      <td>8716.501300</td>\n",
              "      <td>8768.628800</td>\n",
              "      <td>13.722655</td>\n",
              "      <td>8741.672294</td>\n",
              "      <td>0.005945</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>403223 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          BTC_close      BTC_low  ...  BTC_HLPercent  target\n",
              "0        675.500000   675.500000  ...       0.000100       0\n",
              "1        667.000000   667.000000  ...       0.005961       2\n",
              "2        670.100000   670.100000  ...       0.002827       2\n",
              "3        667.000007   667.000007  ...       0.007440       2\n",
              "4        667.000007   667.000007  ...       0.000100       2\n",
              "...             ...          ...  ...            ...     ...\n",
              "403218  8824.457823  8814.000000  ...       0.001597       0\n",
              "403219  8780.138300  8772.888273  ...       0.005680       0\n",
              "403220  8776.869900  8773.414687  ...       0.001887       0\n",
              "403221  8758.555000  8732.293856  ...       0.004627       0\n",
              "403222  8730.000000  8716.501300  ...       0.005945       0\n",
              "\n",
              "[403223 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSqPTNz-m5fo"
      },
      "source": [
        "# Functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V8NN17CqWGA"
      },
      "source": [
        "split df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAoFhaP8qX7P"
      },
      "source": [
        "def splitDf(df):\n",
        "    \n",
        "    res = []\n",
        "    print(\"\")\n",
        "    print(\"splitDf\")\n",
        "    while len(df) >= SEQ_LEN + len(df.columns) -1:\n",
        "        first = df.head(SEQ_LEN + len(df.columns) -1).copy()\n",
        "        first.index = np.arange(0, len(first))\n",
        "        res.append(first)\n",
        "        df = df.tail(len(df) - CANDLES_SHIFT)\n",
        "        df.index = np.arange(0, len(df))\n",
        "\n",
        "    print(\"-done\")\n",
        "    print(\"\")\n",
        "    return res"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v84s4MzlrJex"
      },
      "source": [
        "balance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hddt3uD8rLYE"
      },
      "source": [
        "def balance(dfs):\n",
        "    \n",
        "    buys = []\n",
        "    sells = []\n",
        "    holds = []\n",
        "    for df in dfs:\n",
        "        if df.at[len(df)-1, 'target'] == 0:\n",
        "            sells.append(df)\n",
        "        elif df.at[len(df)-1, 'target'] == 1:\n",
        "            buys.append(df)\n",
        "        else:\n",
        "          holds.append(df)\n",
        "\n",
        "    print(\"before balancing:\")\n",
        "    print(\"buys:\", len(buys), \", sells:\", len(sells), \", holds:\", len(holds))\n",
        "\n",
        "    smallest = min(len(buys), len(sells), len(holds))\n",
        "    buys = buys[:smallest]\n",
        "    sells = sells[:smallest]\n",
        "    holds = holds[:smallest]\n",
        "\n",
        "    dfsBalanced = buys+sells+holds\n",
        "    return dfsBalanced"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puLrLHQCr2IF"
      },
      "source": [
        "preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkiAmxApr4HE"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from tqdm import tqdm\n",
        "\n",
        "def preprocess(dfs):\n",
        "    \n",
        "    for df in dfs:\n",
        "        for col in df.columns:\n",
        "            if col != \"target\":\n",
        "                df[col] = df[col].pct_change()\n",
        "                df.dropna(inplace=True)\n",
        "                df[col] = preprocessing.scale(df[col].values)\n",
        "                df.index = np.arange(0, len(df))\n",
        "\n",
        "    return dfs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtk_1EDotEdd"
      },
      "source": [
        "sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4r2br77tGf-"
      },
      "source": [
        "def buildSequences(dfs):\n",
        "    \n",
        "    sequences = []\n",
        "    for df in dfs:\n",
        "        if(len(df) == SEQ_LEN):\n",
        "            label = df.at[SEQ_LEN-1, 'target']\n",
        "            df = df.iloc[:, :-1]\n",
        "            dfArray = df.values.tolist()\n",
        "            sequences.append([np.array(dfArray), label])\n",
        "    \n",
        "    return sequences"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd640LH1tix2"
      },
      "source": [
        "split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-HIbtKAtj-0"
      },
      "source": [
        "def split(seqWithTarget):\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "    for seq, target in seqWithTarget:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "\n",
        "    return np.array(X),np.array(y)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBfc_4lKnYpv"
      },
      "source": [
        "# DF manipulation, build training sets:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h5xSZxbqNsz"
      },
      "source": [
        "split into dfs with SEQ_LEN rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31jTXjddn04r",
        "outputId": "125ce716-fa5f-4839-fabe-15765768da3e"
      },
      "source": [
        "import numpy as np\n",
        "splittedDfs = splitDf(main_df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "splitDf\n",
            "-done\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdP3PNKHquOk"
      },
      "source": [
        "seperate training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlwWOLZVqudy"
      },
      "source": [
        "dfsTraining = splittedDfs[:(int(len(splittedDfs) * (1-VALIDATION_PCT)))].copy()\n",
        "dfsValidation = splittedDfs[(int(len(splittedDfs) * (1-VALIDATION_PCT))):].copy()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU3bhQJtq1ft"
      },
      "source": [
        "shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhEBABokq1-I"
      },
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(dfsTraining)\n",
        "random.shuffle(dfsValidation)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo4KSJOxrAN0"
      },
      "source": [
        "balance buys/sells/holds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM9MzjVkq2i6",
        "outputId": "0923e1ea-7878-416b-f773-04e1ccdd4df1"
      },
      "source": [
        "dfsTrainingBalanced = balance(dfsTraining)\n",
        "dfsValidationBalanced = dfsValidation # balance(dfsValidation) , validation data does not have to be balanced   "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "buys: 64588 , sells: 63234 , holds: 33369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmvWD-1bBYVH"
      },
      "source": [
        "shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRUhvptlAiVi"
      },
      "source": [
        "random.shuffle(dfsTrainingBalanced)\r\n",
        "random.shuffle(dfsValidationBalanced)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-mePis4rxA1"
      },
      "source": [
        "preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN1lDbSsryQj",
        "outputId": "7089db30-6128-4c3f-a817-c7cc46f93253"
      },
      "source": [
        "dfsTrainingPreprocessed = preprocess(dfsTrainingBalanced)\n",
        "dfsValidationPreprocessed = preprocess(dfsValidationBalanced)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JleCIrlUs98K"
      },
      "source": [
        "build sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GToeQJ4Us_ll"
      },
      "source": [
        "sequencesTraining = buildSequences(dfsTrainingPreprocessed)\n",
        "sequencesValidation = buildSequences(dfsValidationPreprocessed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab82NbyHtU_L"
      },
      "source": [
        "shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uGMjf2k4tV2n"
      },
      "source": [
        "random.shuffle(sequencesTraining)\n",
        "random.shuffle(sequencesValidation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-ygYD9gtbMx"
      },
      "source": [
        "split sequence from label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Cnkgt0utXt1"
      },
      "source": [
        "train_x, train_y = split(sequencesTraining)\n",
        "validation_x, validation_y = split(sequencesValidation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXt32q1Tt39k"
      },
      "source": [
        "# Model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNzTVj7Ue8Vg"
      },
      "source": [
        "hyper parameters bounds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mXBNmip7e7or"
      },
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "batchSize = 32\n",
        "layers = 2\n",
        "nodes = 256\n",
        "denseNodes = 128\n",
        "dropOut = 0.8\n",
        "learningRate = 0.0001\n",
        "decay = 1e-06\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYo0LRjEg2m4"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# model\n",
        "model = Sequential()\n",
        "\n",
        "for _ in range(layers-1):\n",
        "  model.add(LSTM(nodes, activation=\"tanh\", recurrent_activation = 'sigmoid', recurrent_dropout = 0, unroll = False, use_bias = True, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
        "  model.add(Dropout(dropOut))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(nodes, activation=\"tanh\", recurrent_activation = 'sigmoid', recurrent_dropout = 0, unroll = False, use_bias = True, input_shape=(train_x.shape[1:])))\n",
        "model.add(Dropout(dropOut))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(denseNodes, activation=\"relu\"))\n",
        "model.add(Dropout(dropOut))\n",
        "\n",
        "model.add(Dense(3, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "\n",
        "# opt  \n",
        "opt = tf.keras.optimizers.Adam(lr=learningRate, decay=decay)\n",
        "\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer = opt,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "filename = NAME + \"-{epoch:02d}\"\n",
        "filepath = f\"models/{filename}.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1) # saves every epoch\n",
        "\n",
        "\n",
        "# train\n",
        "history = model.fit(\n",
        "  train_x, train_y,\n",
        "  batch_size = batchSize,\n",
        "  epochs = EPOCHS,\n",
        "  validation_data=(validation_x, validation_y),\n",
        "  callbacks=[checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axokGs51bnTA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}